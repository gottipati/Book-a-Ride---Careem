Below are actions performed on application.

*  Customer will open car booking app.

*  App can automatically identify the customer location based on the gps(using geo location api of android or core location api of ios)
   or customer can enter his location manually. The request can be send to server using node.js. Node.js framework very useful for 
   persistent and reliable systems.
   
*  The driver locations are updated to back end server asynchronously using push mechanism(pub-sub systems) with time frame.

*  In real time the server will fetch nearest cabs based on customer location using shortest path algorithm or k-nearest neighbours 
   algorithm. And it will display early time arrival of the cab.
   
 I have implemented above back end solution in java using both Command pattern and simple facade pattern. Command pattern decouples an 
 object making a request from the one that knows how to perform it. This is very good design solution for easy maintenance of multiple 
 services in service oriented architecture. Please find the class diagram("back end java.jpg") in files. The facade pattern defines
 higher level interface to make subsystems easier to use.
 
 The "SQS.java" file can act as Queue for storing actual events running on back end. Each event can be considered as a command. For example
 "BookCab.java", "CancelCab.java", "NearestCabs.java" are actual commands. "BookARide.java" can act as a simple facade for defining 
 higher level functionality of the system. We can use multiple design solutions based on the requirements.
 
 For efficient access of user data and cab data from the data base we can directly store this information as documents representing JSON.
 So that we can effectively retrive the data using object id as a key. For this we can use either MongoDB or CouchDB.
 
 We can also use replication to keep data geographically close to users. So that the data is always available and improve the read 
 throughput. But for very large data sets or very high query throughput replication is not sufficient. We also need to break the data
 into partitions.
 
 The main reason for wanting to partition data is scalability. Different partitions can be placed on different nodes in a shared-nothing
 cluster. Thus a large data set can be distributed across many disks, and the query load can be disributed across many processors. Partitioning 
 is usually combined with replication, so that copies of each partition are stored on multiple nodes. This means that, even though each
 record belongs to exactly one partition, it may still be stored on several different nodes for fault tolerance.
 
 Below are the challenges we have to address if the data is very high.
 
 1) How to address the issue of re-balancing between partitions?
 
 2) How we can handle the service discovery(Request-routing) if data is disributed or partioned across multiple nodes?
 
 
 Below are the design solutions for the above scalability challenges.
 
 1) No matter which partitioning scheme is used, rebalancing is usually expected to meet some minimum requirements.
    
    -> After re-balancing, the load(data storage, read and write requests) should be fairly shared between the nodes in the cluster
    
    -> While re-balancing is happening the database should continue accepting reads and writes.
    
    -> We don't have overload the network by moving more data.
    
    We can address this issue by using multiple methods like fixed number of partitions of the data. Like we have to assign more load to
    powerful machines. Or we can partition dynamically based on the key range(HBase)
 
 2) We have now partitioned our dataset across multiple nodes running on multiple machines. But there remains an open question: when 
    a client wants to make a request, how does it know which node to connect to? As partitions are rebalanced, the assignment of 
    partitions to nodes changes. 
    
    We can address this issue by allow clients to contact any node (e.g. via a round robin load balancer).If that node coincidentally 
    owns the partition to which the request applies, it can handle the request directly. Otherwise it forwards the request to the 
    appropriate node.
    
    Or we can send all requests from clients to a routing tier first, which determines the node that should handle the request and forwards 
    it accordingly. This routing tier does not itself handle any requests, it only acts as a partition aware load balancer
    
 3) We can use caching on data base for efficient and real time processing. For this we can either use redis or memcached. For multiple event
    handling we can create a queue like amazon SQS using command pattern and route the requests to specific services using observer pattern.
    
 4) We can use load balancer like haProxy or nginx to route the requests to specific server and can able to handle users based on geograpy
    and region.
   
 5) We need to provide a strict persistent mechanism to handle multiple events on data base for accurate data update.
 
 We can separately create a data access layer(persistent layer) to handle all the updates related to data base. So that the business or application
 logic does not depend on any particular data base.
 
 We can use no-sql like cassandra for speedup read requests and real time processing from data base.
 
 
 
   
